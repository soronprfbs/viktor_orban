{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Берём список файлов в нашей папке\n",
    "files = [i for i in os.listdir() if i.endswith('txt')]\n",
    "articles = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Создаём словарь, где ключ — газета, а значение — ссылка на статью\n",
    "for file in files:\n",
    "    key = file.replace('.txt', '')\n",
    "    value = []\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            value.append(line.replace('\\n', ''))\n",
    "    articles[key] = value\n",
    "# В txt файлах хранились ссылки на те статьи, которые релевантны поставленной задаче,\n",
    "# к сожалению, они у меня не сохранились, но сохранились excel-файлы, в которых уже собрана информация\n",
    "# по разным изданиям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36'}\n",
    "url_page = '/public/application/cards?SearchString=&Statuses%5B0%5D.Name=%D0%BF%D0%BE%D0%B1%D0%B5%D0%B4%D0%B8%D1%82%D0%B5%D0%BB%D1%8C+%D0%BA%D0%BE%D0%BD%D0%BA%D1%83%D1%80%D1%81%D0%B0&Statuses%5B1%5D.Name=%D0%BD%D0%B0+%D0%BD%D0%B5%D0%B7%D0%B0%D0%B2%D0%B8%D1%81%D0%B8%D0%BC%D0%BE%D0%B9+%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D1%82%D0%B8%D0%B7%D0%B5&Statuses%5B2%5D.Name=%D0%BF%D1%80%D0%BE%D0%B5%D0%BA%D1%82+%D0%BD%D0%B5+%D0%BF%D0%BE%D0%BB%D1%83%D1%87%D0%B8%D0%BB+%D0%BF%D0%BE%D0%B4%D0%B4%D0%B5%D1%80%D0%B6%D0%BA%D1%83&RegionId=&ContestDirectionTenantId=&CompetitionId=9c0a9f89-727c-453e-a7b7-555cded82f57&DateFrom=&DateTo=&Statuses%5B0%5D.Selected=false&Statuses%5B1%5D.Selected=false&Statuses%5B2%5D.Selected=false&page='\n",
    "main = 'https://xn--80afcdbalict6afooklqi5o.xn--p1ai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_year(x):\n",
    "    try:\n",
    "        a = int(re.search('20\\d{2}', x).group())\n",
    "    except:\n",
    "        a = 0\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_content = [] # Здесь мы будем собирать содержание статей\n",
    "\n",
    "# Парсим заголовки и названия статей в rg\n",
    "for value in articles['rg']:\n",
    "    page = requests.get(value, timeout=10, headers=HEADERS)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    title = soup.find_all('title')[0].text\n",
    "    text = soup.find_all('div', {'class': 'b-material-wrapper__text'})[0].text\n",
    "    text = text.replace('\\n-', '').replace('\\n', '').replace('\\xa0', '').replace('\\r', '')\n",
    "    article_content.append([value, title, text])\n",
    "    sleep(3)\n",
    "\n",
    "rg = pd.DataFrame(article_content, columns=['url', 'title', 'content'])\n",
    "rg['year'] = rg['url'].apply(lambda x: to_year(x))\n",
    "rg.to_excel('rg_articles.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_content = []\n",
    "\n",
    "# Парсим заголовки и названия статей в gazeta\n",
    "for value in tqdm(articles['gazeta']):\n",
    "    page = requests.get(value, timeout=20, headers=HEADERS)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    title = soup.find_all('title')[0].text.replace('Новости', '').replace('Газета.Ru', '').replace('|', '').replace('  ', '')\n",
    "    try:\n",
    "        text = soup.find_all('div', {'class': 'article-text-body'})[0].text\n",
    "    except:\n",
    "        try:\n",
    "            text = soup.find_all('div', {'class': 'article_text txt_1'})[0].text\n",
    "        except:\n",
    "            try:\n",
    "                text = soup.find_all('p', {'class': 'txt_3 mb20 mt10'})[0].text\n",
    "            except:\n",
    "                try:\n",
    "                    text = soup.find_all('h2', {'class': 'b_photogallery-intro mt15 mb10'})[0].text\n",
    "                except:\n",
    "                    text = soup.find_all('div', {'class': 'article_text txt_1 pl20'})[0].text\n",
    "    text = text.replace('\\n-', '').replace('\\n', '').replace('\\xa0', '').replace('\\r', '')\n",
    "    article_content.append([value, title[1:-2], text])\n",
    "    sleep(4)\n",
    "\n",
    "gazeta = pd.DataFrame(article_content, columns=['url', 'title', 'content'])\n",
    "gazeta['year'] = gazeta['url'].apply(lambda x: to_year(x))\n",
    "\n",
    "gazeta.loc[54, 'year'] = 2014\n",
    "gazeta.loc[59, 'year'] = 2014\n",
    "gazeta.loc[121, 'year'] = 2015\n",
    "gazeta.loc[270, 'year'] = 2017\n",
    "gazeta.loc[279, 'year'] = 2018\n",
    "gazeta.loc[428, 'year'] = 2020\n",
    "\n",
    "gazeta.to_excel('gazeta_articles.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_content = []\n",
    "\n",
    "# Парсим заголовки и названия статей в mk\n",
    "for value in tqdm(articles['mk'][20:]):\n",
    "    page = requests.get(value, timeout=10, headers=HEADERS)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    title = soup.find_all('title')[0].text\n",
    "    title = title.replace('МК', '')\n",
    "    try:\n",
    "        text = soup.find_all('div', {'class': 'article__body'})[0].text\n",
    "        text = text.split('window.MKInreadsInArticle')[0].replace('  ', '')\n",
    "    except:\n",
    "        text = soup.find_all('div', {'class': 'inread-content'})[0].text\n",
    "    text = text.replace('\\n-', '').replace('\\n', '').replace('\\xa0', '').replace('\\r', '').replace('<br/>', '')\n",
    "    article_content.append([value, title, text])\n",
    "    sleep(3)\n",
    "\n",
    "mk = pd.DataFrame(article_content, columns=['url', 'title', 'content'])\n",
    "mk['year'] = mk['url'].apply(lambda x: to_year(x))\n",
    "mk.to_excel('mk_articles.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_content = []\n",
    "\n",
    "# Парсим заголовки и названия статей в iz\n",
    "for i in range(10, 21):\n",
    "    year = '20' + str(i)\n",
    "    for value in tqdm(articles['iz' + str(i)]):\n",
    "        page = requests.get(value, timeout=10, headers=HEADERS)\n",
    "        soup = BeautifulSoup(page.text, 'lxml')\n",
    "        title = soup.find_all('title')[0].text\n",
    "        text = soup.find_all('div', {'class': 'text-article__inside'})[0].text\n",
    "        text = text.replace('\\n-', '').replace('\\n', '').replace('\\xa0', '').replace('\\r', '').replace('\\t', '')\n",
    "        text = text.split('Поделиться')[0]\n",
    "        article_content.append([value, title, text, int(year)])\n",
    "        sleep(3)\n",
    "\n",
    "iz = pd.DataFrame(article_content, columns=['url', 'title', 'content', 'year'])\n",
    "iz.to_excel('iz_articles.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_content = []\n",
    "\n",
    "# Парсим заголовки и названия статей в ng\n",
    "for i in range(10, 21):\n",
    "    year = '20' + str(i)\n",
    "    for value in tqdm(articles['ng' + str(i)][1:]):\n",
    "        page = requests.get(value, timeout=10, headers=HEADERS)\n",
    "        soup = BeautifulSoup(page.text, 'lxml')\n",
    "        try:\n",
    "            title = soup.find_all('title')[0].text\n",
    "        except:\n",
    "            title = soup.find_all('h1')[0].text\n",
    "        try:\n",
    "            text = soup.find_all('div', {'class': 'news_detail_content'})[0].text\n",
    "        except:\n",
    "            text = soup.find_all('article', {'class': 'typical'})[0].text\n",
    "        text = text.replace('\\n-', '').replace('\\n', '').replace('\\xa0', '').replace('\\r', '').replace('\\t', '')\n",
    "        article_content.append([value, title, text, int(year)])\n",
    "        sleep(3)\n",
    "\n",
    "ng = pd.DataFrame(article_content, columns=['url', 'title', 'content', 'year'])\n",
    "ng.to_excel('ng_articles.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_content = []\n",
    "\n",
    "# Парсим заголовки и названия статей в rbk\n",
    "for i in range(10, 21):\n",
    "    if i == 13:\n",
    "        None\n",
    "    else:\n",
    "        year = '20' + str(i)\n",
    "        for value in tqdm(articles['rbk' + str(i)][1:]):\n",
    "            page = requests.get(value, timeout=10, headers=HEADERS)\n",
    "            soup = BeautifulSoup(page.text, 'lxml')\n",
    "            title = soup.find_all('title')[0].text\n",
    "            soup.find_all('div', {'class': 'article__text article__text_free'})[0].text\n",
    "            text = text.replace('\\n-', '').replace('\\n', '').replace('\\xa0', '').replace('\\r', '').replace('\\t', '')\n",
    "            article_content.append([value, title, text, int(year)])\n",
    "            sleep(3)\n",
    "\n",
    "rbk = pd.DataFrame(article_content, columns=['url', 'title', 'content', 'year'])\n",
    "rbk.to_excel('rbk_articles.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
